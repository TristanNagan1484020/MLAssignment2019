{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tristan Nagan - 1484720\n",
    "Marc Marsden - 1437889\n",
    "Lehyendran Govender - 1106458\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project implemented machine learning methods to differentiate music between different composers. This was achieved by inspection of MIDI metadata and audio features (predominantly consisting of spectral analysis). The first two models were different versions of **Na√Øve Bayes** and the third was **Logistic Regression**, which gives the probability that a certain composition is composed by a certain composer.\n",
    "After passing our data through 3 algorithms, we ran multiple tests using different combinations of composers and features in order to find the combinations that gave us the highest accuracies. \n",
    "\n",
    "\n",
    "Finally we analysed the performance of our algorithms and then added some recommendations for working with such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "This project was based on a public set of classical compositions for piano. The dataset was sourced from http://www.piano-midi.de/. MIDI files were taken as the raw data and **jAudio** (http://jaudio.sourceforge.net/) was used to extract features from the MIDI. There are 127 MIDI files (datapoints) in the dataset. The size of the dataset was increased by splitting the MIDI files into 16 second samples before extracting 13 audio features per file. This gave us a total of 2514 samples.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "The chosen target variable was composer name.\n",
    "\n",
    "| Target Classes \t|\n",
    "|:---------------:\t|\n",
    "| Beethoven       \t|\n",
    "| Chopin          \t|\n",
    "| Mozart          \t|\n",
    "| Schubert        \t|\n",
    "\n",
    "The attributes/features for the data are as follows:\n",
    "\n",
    "The extracted audio attributes/features using **jAudio**:\n",
    "\n",
    "|                  Features                 \t|                                                                                              Description                                                                                             \t|\n",
    "|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\t|\n",
    "| MFCC                                      \t| The Mel-frequency Cepstrum (MFC) is a representation of the short-term power spectrum of a sound, the Mel-frequency Cepstral Coefficients (MFCCs) are coefficients that collectively make up an MFC. \t|\n",
    "| Spectral Flux                             \t| A measure of how quickly the power spectrum of a signal is changing                                                                                                                                  \t|\n",
    "| Compactness                               \t| A measure of the noisiness of a signal. Found by comparing the components of a window's magnitude spectrum with the magnitude spectrum of its neighbouring windows.                                  \t|\n",
    "| Spectral Variability                      \t| The standard deviation of the magnitude spectrum. This is a measure of the variance of a signal's magnitude spectrum                                                                                 \t|\n",
    "| Root Mean Square                          \t| (RMS) is a measure of the power of a signal.                                                                                                                                                                  \t|\n",
    "| Zero Crossings                            \t| The number of times the waveform changed sign. An indication of frequency as well as noisiness.                                                                                                      \t|\n",
    "| Strongest Frequency Via Zero Crossings    \t| The strongest frequency component of a signal, in Hz, found via the number of zero-crossings.                                                                                                        \t|\n",
    "| Strongest Frequency Via Spectral Centroid \t| The strongest frequency component of a signal, in Hz, found via the spectral centroid.                                                                                                               \t|\n",
    "| Strongest Frequency Via FFT Maximum       \t| The strongest frequency component of a signal, in Hz, found via finding the FFT bin with the highest power.                                                                                          \t|\n",
    "| LPC                                       \t| Linear Prediction Coeffecients calculated using autocorrelation and Levinson-Durbin recursion.                                                                                                       \t|\n",
    "| Method of Moments                         \t| Statistical Method of Moments of the Magnitude Spectrum.                                                                                                                                             \t|\n",
    "| Relative Difference Function              \t| Log of the derivative of RMS.  Used for onset detection.                                                                                                                                             \t|\n",
    "| Peak Based Spectral Smoothness            \t| Peak Based Spectral Smoothness is calculated from partials, not frequency bins.                                                                                                                      \t|\n",
    "\n",
    "The extracted audio attributes/features using **Mido**:\n",
    "\n",
    "|                  Features                 \t|                                                                                              Description                                                                                             \t|\n",
    "|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\t|\n",
    "| Key Signature                                     \t| In musical notation, key signature refers to the arrangment of signs such as sharps or flats to indicate its corresponding musical notes \t|\n",
    "| Time Signature                             \t| Tells us how the music is supposed to be counted  |\n",
    "| Mean Tempo                               \t| An average of the tempo of a composition                                 \t|\n",
    "\n",
    "The last 3 attributes were only used used in the **Discrete Na&iuml;ve Bayes** algorithm. Mido is a Python library for for working with MIDI Objects (https://mido.readthedocs.io/en/latest/).\n",
    "\n",
    "Please see Appendix A for an example of a data point for the main methods (**Gaussian Na&iuml;ve Bayes** and **Logistic Regression**) is:\n",
    "\n",
    "\n",
    "An example of a data point for the **Discrete Na&iuml;ve Bayes** algorithm (after feature extraction) is:\n",
    "\n",
    "`['Schubert', 'Ab', '3,4', 1]`\n",
    "\n",
    "## Data Structuring and Normalization\n",
    "\n",
    "The data has been limited to only include the works of **Chopin**, **Mozart**, **Schubert**, and **Beethoven**. This choice was made to narrow the problem space. The data was preprocessed by passing it through **jAudio** to extract the desired features. The size of the dataset was increased by splitting the MIDI files into 16 second samples before extracting the audio features with **jAudio**. The split MIDI  files were not used for the **Discrete Na&iuml;ve Bayes** method since this would have lead to repeated values in the training data rather than an expanded data set.\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "For the **Gaussian Na&iuml;ve Bayes** and **Logistic Regression** methods, which operated on the extracted audio features, the data was split using the `train_test_split` method from the **sklearn** library. This method splits the data randomly into training and testing data according to a given ratio. We used 66.6% of the data for training and 33.3% for testing.\n",
    "\n",
    "For the **Discrete Na&iuml;ve Bayes** method, which operated on the raw MIDI files, two different strategies for splitting the data were implemented. The first strategy was to split the data into 60% training data and 40% test data by picking randomly from the available datapoints. The second strategy was to split the data in the same 60% / 40% ratio but enforced equal representation for all composers in the training data. A comparison of these two strategies is given in the **Discrete Na&iuml;ve Bayes** section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "## Gaussian Na&iuml;ve Bayes \n",
    "\n",
    "This algorithm used the audio features we extracted with **jAudio**.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "The implementation of this algorithm assumed a normal distribution for each feature in the data. The variances and means of these distributions were learned from the training data and then used to calculate the likelihoods for the test data. When performing a classification with this algorithm, the probability of generating a given feature value within a given class needs to be calculated. Since the probability of generating any given feature value in continuous data is zero, the algorithm instead looks at the probability of generating a value within $10^{-9} \\sigma^2$ of the given feature value. This interval seems sufficiently tight to reject false positives but wide enough to mitigate the problem of finding a probability of 0 for all values.\n",
    "\n",
    "### Error On Test Set\n",
    "The data was tested against the individual features to see which were the best. Below are the 7 features that gave the best performances\n",
    "\n",
    "###### 1. MFCC\n",
    "![Confusion matrix for MFCC](report/plots/MFCC.png)\n",
    "Accuracy = 41.63658243080626\n",
    "\n",
    "###### 2. Compactness\n",
    "![Confusion matrix for Compactness](report/plots/Compact.png)\n",
    " Accuracy = 34.89771359807461\n",
    "\n",
    "###### 3. Spectral Flux\n",
    "![Confusion matrix for Spectral Flux](report/plots/SpectralFlux.png)\n",
    "Accuracy = 34.89771359807461\n",
    "\n",
    "###### 4. Peak Speactral Smoothness\n",
    "![Confusion matrix for Peak Speactral Smoothness](report/plots/Smooth.png)\n",
    "Accuracy = 34.05535499398315\n",
    "\n",
    "###### 5. Method of Moments\n",
    "![Confusion matrix for Method of Moments](report/plots/Moments.png)\n",
    "Accuracy = 32.61131167268351\n",
    "\n",
    "###### 6. Strongest Frequency Via FFT Maximum\n",
    "![Confusion matrix for Strongest Frequency Via FFT Maximum](report/plots/FFTMax.png)\n",
    "Accuracy = 31.64861612515042\n",
    "\n",
    "###### 7. Root Mean Squared\n",
    "![Confusion matrix for RMS](report/plots/RMS.png)\n",
    "Accuracy = 30.20457280385078\n",
    "\n",
    "###### All Features\n",
    "![Confusion matrix for All features](report/plots/NB_All.png)\n",
    "Accuracy = 36.22141997593261\n",
    "\n",
    "From the results it is easy to see that MFCC is a better feature when it is the only one used versus using all the features. This could be due to the fact that the data set was not big enough or that some of the features were not very good choices for distinguishing between different composers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Na&iuml;ve Bayes\n",
    "\n",
    "This algorithm used the metadata and message data directly from the MIDI files to predict the composer of a given piece. The reason for this implementation was to try and perform the classification with discrete data straight from the MIDI files in order to avoid inaccuracies and complications introduced in the processing of the continuous; multi-dimensional features we extracted using **jAudio**. Although it is not a particularly interesting way to view the data (since the composer of a piece is usually specified in the metadata of a MIDI file) it may provide an interesting contrast to the other method in terms of results.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "A straightforward implementation of **Na&iuml;ve Bayes** with Laplace smoothing. The features used in this algorithm were extracted directly from the MIDI files and were chosen for simplicity's sake. Due to inconsistent labelling of composer names in the MIDI files, some datapoints got lost in the data preparation process and thus were not used for this algorithm. Since this algorithm used a different set of features from the other two, a brief description of these features is given below:\n",
    "\n",
    "#### Key Signature\n",
    "\n",
    "The first key signature given in the MIDI file. Subsequent key signature changes were ignored for simplicity. Key signatures are represented as strings such as `C` or `Ab`. Due to inconsistencies in the representation of key MIDI metadata, some key signatures were represented twice in two different ways (for example **F<sup>#</sup>** and **G<sup>b</sup>** which are in fact the same key).\n",
    "\n",
    "![Occurences of different key signatures in the data](report/plots/key_freq.png)\n",
    "\n",
    "#### Time Signature\n",
    "\n",
    "The first time signature given in the MIDI file. Subsequent time signature changes were ignored since they are fairly uncommon in the dataset, and would simply complicate the problem. Time signatures are represented as strings such as `3,4` or `5,4`.\n",
    "\n",
    "#### Mean Tempo\n",
    "\n",
    "An average of the tempo throughout the whole piece. This is measured in ticks. The mean tempo was then discretized by finding the mean $\\mu$ and variance $\\sigma^2$ of the mean tempos across all data points and then turning them into discrete values according to the rule:\n",
    "\n",
    "$T_{class} = 0 \\quad$ if $T_{value} < \\mu - \\sigma^2$, Low tempo.\n",
    "\n",
    "$T_{class} = 1 \\quad$ if $\\mu - \\sigma^2 <= T_{value} <= \\mu + \\sigma^2$, Mid tempo.\n",
    "\n",
    "$T_{class} = 2 \\quad$ if $T_{value} < \\mu + \\sigma^2$, High tempo.\n",
    "\n",
    "High mean tempos were absent in the dataset.\n",
    "\n",
    "### Error On Test Set\n",
    "\n",
    "When the data was split randomly into training and testing data, one of the composers tended to be over-represented in the training data, leading to the model favouring that composer in the prediction. When the training data was chosen in a favourable way, the results were fairly good for the two composers case.  \n",
    "\n",
    "\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (two composers). First result.](report/plots/confusion_discNB_01.png)\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (two composers). Second result.](report/plots/confusion_discNB_01b.png)\n",
    "\n",
    "The problem got worse when all four composers were present in the data. Chopin and Schubert are hugely overrepresented, so the random process tended to favour them more.\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (four composers).](report/plots/confusion_discNB_02.png)\n",
    "\n",
    "When the number of datapoints for each composer in the training data was standardized, it improved the results of this algorithm slightly for the two composers case but didn't improve the results for the four composers case.\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on an equalized test set (two composers).](report/plots/confusion_discNB_03.png)\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on an equalized test set (four composers).](report/plots/confusion_discNB_04.png)\n",
    "\n",
    "The lack of improvement in the four composers case serves to demonstrate that either the dataset is too small or these features are not sufficient to characterise a composer's style and thus are a poor choice compared to the extracted audio features used in the other algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This algorithm used the same data as the **Gaussian Na&iuml;ve Bayes** algorithm. Logistic regression gave us a more discriminative method in comparison to **Na&iuml;ve Bayes** which is more generative. Thus, this led to a more continuous measure that provides us with a probability which represents the likelihood that a piano composition belongs to a certain composer. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implemented multiclass classification using the ‚ÄúOne vs Rest(OVR)‚Äù method. Since we have 4 target classes, namely; **Beethoven**, **Chopin**, **Mozart** and **Schubert**, we first treated **Beethoven** as one class and **Chopin**, **Mozart** and **Schubert** as the other class and then ran our logistic regression model. We repeated this process for each composer and ended up with 4 different, independent logistic regressions. \n",
    "\n",
    "We then had 4 classifiers to use for prediction where each one gave us a probability of its associated class. The most probable class is then the one that yields the highest probability.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The learning rate, $\\alpha$, that we used initially had a value of 0.1. This gave us an accuracy of 49.09747%. We then incrementally increased its value by 0.1 and discovered that the accuracy of the model increased as we approached 1. At $\\alpha$ = 1, the models accuracy was 51.98555%. Despite $\\alpha$ being relatively large, it did not produce a sub-optimal set of weights.\n",
    "\n",
    "##### $\\alpha$ = 0.1\n",
    "![Logistic regression for aplha = 0.1.](report/plots/LRalpha1.png)\n",
    "\n",
    "\n",
    "##### $\\alpha$ = 1\n",
    "![Logistic regression for aplha = 1.0.](report/plots/LRalpha2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results\n",
    "\n",
    "The **Gaussian Na&iuml;ve Bayes** algorithm performed sub-optimally compared to **Logistic Regression** but a lot better than **Discrete Na&iuml;ve Bayes**. As stated above, a possible reason for the poor performance of **Gaussian Na&iuml;ve Bayes** is the lack of data or the fact that some of the extracted features did not correlate sufficiently.\n",
    "\n",
    "The **Discrete Na&iuml;ve Bayes** algorithm performed the worst by far. This is most likely due to the limited number of data points and the lack of readily available features in the MIDI files.\n",
    "\n",
    "## Best Possible Performance\n",
    "\n",
    "**Logistic Regression** was the best performer out of the 3 algorithms used. The difference in accuracy was approximately 15%. The main possible reason for the algorithm performing better is due to the lack of bias and high variance within our dataset and the algorithms itself. **Logistic Regression** was also less computationality heavy compared to the other algorithms which means it ran a lot quicker. \n",
    "\n",
    "In conclusion, **Logistic Regression** performed better than complete randomness. Thus we were able to predict if certain musical pieces were composed by **Beethoven**, **Chopin**, **Mozart** and **Schubert** using the extracted features. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Recommendations to Others Working on This Data\n",
    "\n",
    "- Don't use the raw MIDI data. Extracting established audio features is much more effective.\n",
    "- Split the compositions into short snippets to increase the size of the dataset.\n",
    "- Avoid audio features that give a huge number of values such as **Power Spectrum** since these will only add to *the curse of dimensionality*.\n",
    "- Make sure to use MFCC as a feature, it seems to be a strong predictor.\n",
    "- Due to the similarity between composers, this made it more difficult for our alrgorithms to properly identify the musical compositions. Therefore, choosing composers from different eras of classifical music could possibly give a higher accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix A\n",
    "\n",
    "<section start=\"16.384\" stop=\"32.7679375\">\n",
    "        <feature>\n",
    "            <name>Spectral Flux</name>\n",
    "            <v>8.51E-6</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Compactness</name>\n",
    "            <v>7.406E5</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Spectral Variability</name>\n",
    "            <v>8.891E-6</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Root Mean Square</name>\n",
    "            <v>1.367E-2</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Zero Crossings</name>\n",
    "            <v>1.631E4</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Strongest Frequency Via Zero Crossings</name>\n",
    "            <v>4.976E2</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Strongest Frequency Via Spectral Centroid</name>\n",
    "            <v>5.82E2</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Strongest Frequency Via FFT Maximum</name>\n",
    "            <v>6.594E2</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>MFCC</name>\n",
    "            <v>-1.225E2</v>\n",
    "            <v>1.344E1</v>\n",
    "            <v>-1.062E1</v>\n",
    "            <v>-3.897E0</v>\n",
    "            <v>-3.706E0</v>\n",
    "            <v>1.668E0</v>\n",
    "            <v>2.414E0</v>\n",
    "            <v>2.589E0</v>\n",
    "            <v>-1.625E0</v>\n",
    "            <v>3.015E-1</v>\n",
    "            <v>1.797E0</v>\n",
    "            <v>1.988E0</v>\n",
    "            <v>-1.681E-1</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>LPC</name>\n",
    "            <v>-9.465E-1</v>\n",
    "            <v>9.681E-1</v>\n",
    "            <v>-5.564E-1</v>\n",
    "            <v>5.075E-1</v>\n",
    "            <v>-3.432E-1</v>\n",
    "            <v>3.585E-1</v>\n",
    "            <v>-1.659E-1</v>\n",
    "            <v>-2.825E-2</v>\n",
    "            <v>-3.143E-2</v>\n",
    "            <v>0E0</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Method of Moments</name>\n",
    "            <v>2.216E-1</v>\n",
    "            <v>1.208E4</v>\n",
    "            <v>1.11E8</v>\n",
    "            <v>4.405E12</v>\n",
    "            <v>3.371E17</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Partial Based Spectral Centroid</name>\n",
    "            <v>4.152E1</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Partial Based Spectral Flux</name>\n",
    "            <v>-6.508E-3</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Peak Based Spectral Smoothness</name>\n",
    "            <v>4.624E2</v>\n",
    "        </feature>\n",
    "        <feature>\n",
    "            <name>Relative Difference Function</name>\n",
    "            <v>-4.923E0</v>\n",
    "        </feature>\n",
    "    </section>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
