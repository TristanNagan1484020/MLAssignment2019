{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This project implemented machine learning methods to differentiate between music by different composers. This was achieved by inspection of MIDI metadata and audio features (predominantly consisting of spectral analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "This project was based on a public set of classical compositions for piano. The dataset was sourced from http://www.piano-midi.de/. MIDI files were taken as the raw data and **jAudio** (http://jaudio.sourceforge.net/) was used to extract features from the MIDI. There are 127 MIDI files (datapoints) in the dataset. The size of the dataset was increased by splitting the MIDI files into 16 second samples before extracting 13 audio features per file. This gave us a total of (number) samples. See datapoint example in appendix(include xml file with one midi).\n",
    "\n",
    "## Attributes\n",
    "\n",
    "The chosen target variable was composer name.\n",
    "\n",
    "| Target Classes \t|\n",
    "|:---------------:\t|\n",
    "| Beethoven       \t|\n",
    "| Chopin          \t|\n",
    "| Mozart          \t|\n",
    "| Schubert        \t|\n",
    "\n",
    "The attributes/features for the data are as follows:\n",
    "\n",
    "The extracted audio attributes/features using **jAudio**:\n",
    "\n",
    "|                  Features                 \t|                                                                                              Description                                                                                             \t|\n",
    "|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\t|\n",
    "| MFCC                                      \t| The Mel-frequency Cepstrum (MFC) is a representation of the short-term power spectrum of a sound, the Mel-frequency Cepstral Coefficients (MFCCs) are coefficients that collectively make up an MFC. \t|\n",
    "| Spectral Flux                             \t| A measure of how quickly the power spectrum of a signal is changing                                                                                                                                  \t|\n",
    "| Compactness                               \t| A measure of the noisiness of a signal. Found by comparing the components of a window's magnitude spectrum with the magnitude spectrum of its neighbouring windows.                                  \t|\n",
    "| Spectral Variability                      \t| The standard deviation of the magnitude spectrum. This is a measure of the variance of a signal's magnitude spectrum                                                                                 \t|\n",
    "| Root Mean Square                          \t| A measure of the power of a signal.                                                                                                                                                                  \t|\n",
    "| Zero Crossings                            \t| The number of times the waveform changed sign. An indication of frequency as well as noisiness.                                                                                                      \t|\n",
    "| Strongest Frequency Via Zero Crossings    \t| The strongest frequency component of a signal, in Hz, found via the number of zero-crossings.                                                                                                        \t|\n",
    "| Strongest Frequency Via Spectral Centroid \t| The strongest frequency component of a signal, in Hz, found via the spectral centroid.                                                                                                               \t|\n",
    "| Strongest Frequency Via FFT Maximum       \t| The strongest frequency component of a signal, in Hz, found via finding the FFT bin with the highest power.                                                                                          \t|\n",
    "| LPC                                       \t| Linear Prediction Coeffecients calculated using autocorrelation and Levinson-Durbin recursion.                                                                                                       \t|\n",
    "| Method of Moments                         \t| Statistical Method of Moments of the Magnitude Spectrum.                                                                                                                                             \t|\n",
    "| Relative Difference Function              \t| log of the derivative of RMS.  Used for onset detection.                                                                                                                                             \t|\n",
    "| Peak Based Spectral Smoothness            \t| Peak Based Spectral Smoothness is calculated from partials, not frequency bins.                                                                                                                      \t|\n",
    "\n",
    "The extracted audio attributes/features using **Mido**:\n",
    "\n",
    "|                  Features                 \t|                                                                                              Description                                                                                             \t|\n",
    "|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\t|\n",
    "| Key Signature                                     \t| In musical notation, key signature refers to the arrangment of signs such as sharps or flats to indicate its corresponding musical notes \t|\n",
    "| Time Signature                             \t| Tells us how the music is supposed to be counted  |\n",
    "| Mean Tempo                               \t| An average of the tempo of a composition                                 \t|\n",
    "\n",
    "The last 3 attributes were only used used in the **Discrete Na&iuml;ve Bayes** algorithm. Mido is a Python library for for working with MIDI Objects (https://mido.readthedocs.io/en/latest/).\n",
    "\n",
    "An example of a data point for the main methods (**Gaussian Na&iuml;ve Bayes** and **Logistic Regression**) is:\n",
    "\n",
    "`[]`\n",
    "\n",
    "An example of a data point for the **Discrete Na&iuml;ve Bayes** algorithm (after feature extraction) is:\n",
    "\n",
    "`['Schubert', 'Ab', '3,4', 1]`\n",
    "\n",
    "## Data Structuring and Normalization\n",
    "\n",
    "The data has been limited to only include the works of **Chopin**, **Mozart**, **Schubert**, and **Beethoven**. This choice was made to narrow the problem space. The data was preprocessed by passing it through **jAudio** to extract the desired features. The size of the dataset was increased by splitting the MIDI files into 16 second samples before extracting the audio features with **jAudio**. The split MIDI  files were not used for the **Discrete Na&iuml;ve Bayes** method since this would have lead to repeated values in the training data rather than an expanded data set.\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "For the **Gaussian Na&iuml;ve Bayes** and **Logistic Regression** methods, which operated on the extracted audio features, the data was split using the `train_test_split` method from the **sklearn** library. This method splits the data randomly into training and testing data according to a given ratio. We used 66.6% of the data for training and 33.3% for testing.\n",
    "\n",
    "For the **Discrete Na&iuml;ve Bayes** method, which operated on the raw MIDI files, two different strategies for splitting the data were implemented. The first strategy was to split the data into 60% training data and 40% test data by picking randomly from the available datapoints. The second strategy split the data in the same 60% / 40% ratio but enforced equal representation for all composers in the training data. A comparison of these two strategies is given in the **Discrete Na&iuml;ve Bayes** section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "## Gaussian Na&iuml;ve Bayes \n",
    "\n",
    "This algorithm used the audio features we extracted with **jAudio**.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "The implementation of this algorithm assumed a normal distribution for each feature in the data. The variances and means of these distributions was learned from the training data and then used to calculate the likelihoods for the test data. When performing a classification with this algorithm, the probability of generating a given feature value within a given class needs to be calculated. Since the probability of generating any given feature value in continuous data is zero, the algorithm instead looks at the probability of generating a value within $10^{-9} \\sigma^2$ of the given feature value. This interval seems sufficiently tight to reject false positives but wide enough to mitigate the problem of finding a probability of 0 for all values.\n",
    "\n",
    "### Error On Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Na&iuml;ve Bayes\n",
    "\n",
    "This algorithm used the metadata and message data directly from the MIDI files to predict the composer of a given piece. The reason for this implementation was to try and perform the classification with discrete data straight from the MIDI files in order to avoid inaccuracies and complications introduced in the processing of the continuous; multi-dimensional features we extracted using jAudio. Although it is not a particularly interesting way to view the data (since the composer of a piece is usually specified in the metadata of a MIDI file) ~it may provide an interesting contrast to the other method in terms of results.~\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "A straightforward implementation of Na&iuml;ve Bayes with Laplace smoothing. The features used in this algorithm were extracted directly from the MIDI files and were chosen for simplicity's sake. Due to inconsistent labelling of composer names in the MIDI files, some datapoints got lost in the data preparation process and thus were not used for this algorithm. Since this algorithm used a different set of features from the other two, a brief description of these features is given below:\n",
    "\n",
    "#### Key Signature\n",
    "\n",
    "The first key signature given in the MIDI file. Subsequent key signature changes were ignored for simplicity. Key signatures are represented as strings such as `C` or `Ab`. Due to inconsistencies in the representation of key MIDI metadata, some key signatures were represented twice in two different ways (for example **F<sup>#</sup>** and **G<sup>b</sup>** which are in fact the same key).\n",
    "\n",
    "![Occurences of different key signatures in the data](report/plots/key_freq.png)\n",
    "\n",
    "#### Time Signature\n",
    "\n",
    "The first time signature given in the MIDI file. Subsequent time signature changes were ignored since they are fairly uncommon in the dataset, and would simply complicate the problem. Time signatures are represented as strings such as `3,4` or `5,4`.\n",
    "\n",
    "#### Mean Tempo\n",
    "\n",
    "An average of the tempo throughout the whole piece. This is measured in ticks. The mean tempo was then discretized by finding the mean $\\mu$ and variance $\\sigma^2$ of the mean tempos across all data points and then turning them into discrete values according to the rule:\n",
    "\n",
    "$T_{class} = 0 \\quad$ if $T_{value} < \\mu - \\sigma^2$, Low tempo.\n",
    "\n",
    "$T_{class} = 1 \\quad$ if $\\mu - \\sigma^2 <= T_{value} <= \\mu + \\sigma^2$, Mid tempo.\n",
    "\n",
    "$T_{class} = 2 \\quad$ if $T_{value} < \\mu + \\sigma^2$, High tempo.\n",
    "\n",
    "High mean tempos were absent in the dataset.\n",
    "\n",
    "### Error On Test Set\n",
    "\n",
    "When the data was split randomly into training and testing data, one of the composers tended to be over-represented in the training data, leading to the model favouring that composer in the prediction. When the training data was chosen in a favourable way, the results were fairly good for the two composers case.  \n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (two composers). First result.](report/plots/confusion_discNB_01.png)\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (two composers). Second result.](report/plots/confusion_discNB_01b.png)\n",
    "\n",
    "The problem got worse when all four composers were present in the data. Chopin and Schubert are hugely overrepresented, so the random process tended to favour them more.\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on a random test set (four composers).](report/plots/confusion_discNB_02.png)\n",
    "\n",
    "When the number of datapoints for each composer in the training data was standardized, it improved the results of this algorithm slightly for the two composers case but didn't improve the results for the four composers case.\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on an equalized test set (two composers).](report/plots/confusion_discNB_03.png)\n",
    "\n",
    "![Discrete Na&iuml;ve Bayes on an equalized test set (four composers).](report/plots/confusion_discNB_04.png)\n",
    "\n",
    "The lack of improvement in the four composers case serves to demonstrate that either the dataset is too small or these features are not sufficient to characterise a composer's style and thus are a poor choice compared to the extracted audio features used in the other algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This algorithm used the same data as the Gaussian Naive Bayes algorithm. Logistic regression gave us a more discriminative method in comparison to Naive Bayes which is more generative. Thus, this led to a more continuous measure that provides us with a probability which represents the likelihood that a piano composition belongs to a certain composer. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implemented multiclass classification using the “One vs Rest(OVR)” method. Since we have 4 target classes, namely; Beethoven, Chopin, Mozart and Schubert, we first treated Beethoven as one class and Chopin, Mozart and Schubert as the other class and then ran our logistic regression model. We repeated this process for each composer and ended up with 4 different, independent logistic regressions. \n",
    "\n",
    "We then had 4 classifiers to use for prediction where each one gave us a probability of its associated class. The most probable class is then the one that yields the highest probability.\n",
    "\n",
    "\n",
    "### Error On Test Set\n",
    "\n",
    "Insert confusion matrix/matrices and explain\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The learning rate, $\\alpha$, that we used initially had a value of 0.1. This gave us an accuracy of (percentage). We then incrementally increased its value by 0.1 and discovered that the accuracy of the model increased as we approached 1. At $\\alpha$ = 1, the models accuracy was (percentage). Despite $\\alpha$ being relatively large, it (did or did not) produce a sub-optimal set of weights. (Show $\\alpha$ = 1 weights vs $\\alpha$ = 2 weights) \n",
    "\n",
    "Was it a goo d choice? Growth of weghts?\n",
    "\n",
    "**NB** Results: explain some features are worse than others, mfcc was the strongest etc (include confusion matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results\n",
    "\n",
    "The **Discrete Na&iuml;ve Bayes** algorithm performed the worst by far. This is most likely due to the limited number of data points and the lack of readily available features in the MIDI files.\n",
    "\n",
    "## Best Possible Performance\n",
    "\n",
    "## Recommendations to Others Working on This Data\n",
    "\n",
    "- Don't use the raw MIDI data. Extracting established audio features is much more effective.\n",
    "- Split the compositions into short snippets to increase the size of the dataset.\n",
    "- Avoid audio features that give a huge number of values such as **Power Spectrum** since these will only add to *the curse of dimensionality*.\n",
    "- Make sure to use MFCC as a feature, it seems to be a strong predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
